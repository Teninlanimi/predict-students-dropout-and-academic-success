{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cade34",
   "metadata": {},
   "source": [
    "# Deploy pipeline: load, preprocess, train, and export model\n",
    "\n",
    "This notebook loads `data.csv` (delimiter autodetected), applies the same column drops used in your training notebook, coerces numeric columns, trains a StandardScaler+RandomForest pipeline, and writes `multiclass_classification_model.pkl` and `final_feature_list.txt` to the project root so `app.py` can load them directly.\n",
    "\n",
    "Run cells in order. If your CSV uses `;` as delimiter, the loader will detect it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764cd8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports\n",
    "import io\n",
    "import os\n",
    "import csv\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db571f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated f-string literal (detected at line 11) (1881680382.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f'Auto-detected delimiter:\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated f-string literal (detected at line 11)\n"
     ]
    }
   ],
   "source": [
    "# 2) Robust CSV loader (auto-detect delimiter)\n",
    "csv_path = 'data.csv'\n",
    "assert os.path.exists(csv_path), f'{csv_path} not found in repo root'\n",
    "\n",
    "# Read a sample to sniff the delimiter\n",
    "with open(csv_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    sample = f.read(8192)\n",
    "    dialect = csv.Sniffer().sniff(sample, delimiters=[',',';','\t','|'])\n",
    "    delim = dialect.delimiter\n",
    "\n",
    "print(f'Auto-detected delimiter: \n",
    "')\n",
    "\n",
    "df = pd.read_csv(csv_path, sep=delim, engine='python')\n",
    "print('Data loaded. Shape:', df.shape)\n",
    "print('Columns (first 30):', df.columns.tolist()[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Clean column names and create target mapping\n",
    "df.columns = df.columns.str.strip()\n",
    "# Ensure Target exists\n",
    "if 'Target' not in df.columns:\n",
    "    raise RuntimeError('Target column not found in data.csv')\n",
    "\n",
    "# Map Target to numeric consistent with app.py and notebook: Enrolled=0, Dropout=1, Graduate=2\n",
    "df['Target_binary'] = df['Target'].replace({'Graduate': 2, 'Dropout': 1, 'Enrolled': 0})\n",
    "print('Target mapping done. Unique values:', df['Target'].unique(), '->', df['Target_binary'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Drop the columns the original training notebook removed (safe drop)\n",
    "features_to_drop = [\n",
    "    'Application mode', 'Application order', 'Previous qualification', 'Previous qualification (grade)',\n",
    "    \n",
    ", \n",
    ", \n",
    ", \n",
    ",\n",
    "    'Admission grade', 'Debtor', 'International', 'Curricular units 1st sem (credited)',\n",
    "    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (without evaluations)',\n",
    "    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (evaluations)',\n",
    "    'Curricular units 2nd sem (without evaluations)'\n",
    "]\n",
    "# Build feature matrix X and target y (do not include Target itself)\n",
    "X = df.drop(columns=['Target', 'Target_binary'], errors='ignore')\n",
    "existing_features_to_drop = [c for c in features_to_drop if c in X.columns]\n",
    "print(f'Dropping {len(existing_features_to_drop)} columns: {existing_features_to_drop}')\n",
    "X = X.drop(columns=existing_features_to_drop)\n",
    "y = df.loc[X.index, 'Target_binary']\n",
    "print('Remaining features count:', X.shape[1])\n",
    "print('Remaining feature list (first 50):', X.columns.tolist()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5084e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Coerce numeric-like columns to numeric and handle missing values\n",
    "# Try to convert every column to numeric where possible; keep booleans or categorical as-is if conversion fails\n",
    "non_numeric_cols = []\n",
    "for col in X.columns:\n",
    "    # if dtype is object, try to coerce to numeric\n",
    "    if X[col].dtype == 'object':\n",
    "        coerced = pd.to_numeric(X[col].str.replace(',', '.').astype(str).str.strip(), errors='coerce')\n",
    "        # If many values converted (not all NaN), use coerced numeric; otherwise leave original\n",
    "        if coerced.notna().sum() > 0:\n",
    "            X[col] = coerced\n",
    "        else:\n",
    "            non_numeric_cols.append(col)\n",
    "\n",
    "print('Non-numeric columns remaining (treated as categorical/text):', non_numeric_cols)\n",
    "\n",
    "# After coercion, drop rows with any NaNs to keep simple and consistent for training\n",
    "nan_counts = X.isnull().sum().sum()\n",
    "print('Total NaN values in features after coercion:', nan_counts)\n",
    "if nan_counts > 0:\n",
    "    X_before = X.shape[0]\n",
    "    mask = X.notnull().all(axis=1)\n",
    "    X = X.loc[mask].copy()\n",
    "    y = y.loc[X.index].copy()\n",
    "    print(f'Dropped {X_before - X.shape[0]} rows with NaNs; new shape: {X.shape}')\n",
    "else:\n",
    "    print('No NaNs to drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ac9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Quick train/validation split to check model performance, then train on full data and export\n",
    "if X.shape[0] < 10:\n",
    "    raise RuntimeError('Not enough rows after cleaning to train model')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print('Train/Test shapes:', X_train.shape, X_test.shape)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print('Validation accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Refit on full data for export to maximize data used\n",
    "pipeline_full = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, class_weight='balanced'))\n",
    "])\n",
    "pipeline_full.fit(X, y)\n",
    "print('Trained pipeline on full data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Save the pipeline and final feature list (in the project root so app.py can load them)\n",
    "model_path = 'multiclass_classification_model.pkl'\n",
    "joblib.dump(pipeline_full, model_path)\n",
    "print('Saved model to', model_path)\n",
    "\n",
    "# Save feature list preserving column order\n",
    "with open('final_feature_list.txt', 'w', encoding='utf-8') as f:\n",
    "    for feat in X.columns:\n",
    "        f.write(f'{feat}\n",
    "')\n",
    "print('Saved final_feature_list.txt with', len(X.columns), 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Quick load test: load saved model and predict on first row (if any)\n",
    "loaded = joblib.load('multiclass_classification_model.pkl')\n",
    "print('Loaded model. Classes:', getattr(loaded, 'classes_', None))\n",
    "if X.shape[0] > 0:\n",
    "    sample = X.iloc[[0]]\n",
    "    print('Sample shape:', sample.shape)\n",
    "    pred = loaded.predict(sample)[0]\n",
    "    proba = loaded.predict_proba(sample)[0]\n",
    "    print('Prediction:', pred)\n",
    "    print('Probabilities:', proba)\n",
    "else:\n",
    "    print('No sample to test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7eb1c-01ef-4f32-b50c-69b30b32c2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91d093-fcde-4d12-b20c-8f2c372539b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6baca3d-eef7-4057-862d-16fcaa64978d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
